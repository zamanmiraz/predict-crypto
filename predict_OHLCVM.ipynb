{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd37878b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.0000\n",
      "RÂ² Score: 0.9962\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv('Dataset\\doge_dataset_day_ohlcvm.csv')\n",
    "\n",
    "# Drop rows with missing values\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Features and target\n",
    "X = df[['open', 'high', 'low', 'volume_DOGE', 'market_cap', 'volume']]  # You can add more features if available\n",
    "y = df['close']  # Target variable: closing price\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize and train the model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluation\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"RÂ² Score: {r2:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056c0ef4",
   "metadata": {},
   "source": [
    "## Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "290d3d70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 open     close      high       low  volume_DOGE  market_cap  \\\n",
      "open         1.000000  0.995383  0.996608  0.996618     0.068783    0.748095   \n",
      "close        0.995383  1.000000  0.998233  0.996842     0.079557    0.748373   \n",
      "high         0.996608  0.998233  1.000000  0.994104     0.088358    0.740723   \n",
      "low          0.996618  0.996842  0.994104  1.000000     0.060686    0.757410   \n",
      "volume_DOGE  0.068783  0.079557  0.088358  0.060686     1.000000    0.066979   \n",
      "market_cap   0.748095  0.748373  0.740723  0.757410     0.066979    1.000000   \n",
      "volume       0.724094  0.727230  0.731679  0.719526     0.219756    0.531926   \n",
      "\n",
      "               volume  \n",
      "open         0.724094  \n",
      "close        0.727230  \n",
      "high         0.731679  \n",
      "low          0.719526  \n",
      "volume_DOGE  0.219756  \n",
      "market_cap   0.531926  \n",
      "volume       1.000000  \n"
     ]
    }
   ],
   "source": [
    "print(df[['open', 'close', 'high', 'low', 'volume_DOGE', 'market_cap', 'volume']].corr())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb9b383",
   "metadata": {},
   "source": [
    "From the correlation matrix, we can see that market cap and price is correlated. Now there are two options:\n",
    "- Building a time aware model: Model depend on the past data instead of the derived current values.\n",
    "- Remove market cap from the features\n",
    "I will focus more one first option as it could be more realistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0506070e",
   "metadata": {},
   "source": [
    "# Building a time-aware model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad685f45",
   "metadata": {},
   "source": [
    "## Linear Regression, Random Forest, XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757543dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Evaluating models with 1-day lag features:\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from src import lag_features\n",
    "from src.tuning import tune_random_forest, tune_xgboost, tune_lightgbm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# --- Loop through different lag settings ---\n",
    "for lag in range(1, 8):\n",
    "    print(f\"\\nðŸ“Š Evaluating models with {lag}-day lag features:\")\n",
    "    X_train, X_test, y_train, y_test = lag_features.create_lag_features(lag)\n",
    "\n",
    "    # Tune hyperparameters for Random Forest, XGBoost, and LightGBM\n",
    "    best_rf, mse_rf, r2_rf = tune_random_forest(X_train, y_train, X_test, y_test)\n",
    "    best_xgb, mse_xgb, r2_xgb = tune_xgboost(X_train, y_train, X_test, y_test)\n",
    "    best_lgb, mse_lgb, r2_lgb = tune_lightgbm(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    # --- Train Models ---\n",
    "    models = {\n",
    "        \"Linear Regression\": LinearRegression(),\n",
    "        \"Random Forest\": best_rf,\n",
    "        \"XGBoost\": best_xgb,\n",
    "        \"LightGBM\": best_lgb\n",
    "    }\n",
    "\n",
    "    predictions = {}\n",
    "    for name, model in models.items():\n",
    "        model.fit(X_train, y_train)\n",
    "        preds = model.predict(X_test)\n",
    "        predictions[name] = preds\n",
    "        mse = mean_squared_error(y_test, preds)\n",
    "        r2 = r2_score(y_test, preds)\n",
    "        print(f\"{name} - MSE: {mse:.6f}, RÂ²: {r2:.4f}\")\n",
    "\n",
    "    # --- Plot Predictions ---\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(y_test.values, label='Actual Price', color='black')\n",
    "\n",
    "    for name, preds in predictions.items():\n",
    "        linestyle = {\n",
    "            \"Linear Regression\": \"--\",\n",
    "            \"Random Forest\": \"-.\",\n",
    "            \"XGBoost\": \":\",\n",
    "            \"LightGBM\": \"-\"\n",
    "        }[name]\n",
    "        color = {\n",
    "            \"Linear Regression\": \"red\",\n",
    "            \"Random Forest\": \"green\",\n",
    "            \"XGBoost\": \"orange\",\n",
    "            \"LightGBM\": \"purple\"\n",
    "        }[name]\n",
    "        plt.plot(preds, label=name, linestyle=linestyle, color=color)\n",
    "\n",
    "    plt.title(f'ðŸ“ˆ Actual vs Predicted Dogecoin Prices ({lag}-Day Lag)')\n",
    "    plt.xlabel('Test Sample Index')\n",
    "    plt.ylabel('Price (USD)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    # plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecd16b9",
   "metadata": {},
   "source": [
    "# Tuning\n",
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab6aabfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Random Forest Parameters: {'n_estimators': 200, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'sqrt', 'max_depth': 10}\n",
      "Mean Squared Error: 0.0007\n",
      "RÂ² Score: 0.8873\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "param_dist_rf = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [None, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "    'max_features': ['sqrt', 'log2', None] \n",
    "}\n",
    "\n",
    "\n",
    "rf_random = RandomizedSearchCV(\n",
    "    RandomForestRegressor(random_state=42),\n",
    "    param_distributions=param_dist_rf,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "rf_random.fit(X_train, y_train)\n",
    "best_rf = rf_random.best_estimator_\n",
    "y_pred_random = best_rf.predict(X_test)\n",
    "# Evaluation\n",
    "mse_rf = mean_squared_error(y_test, y_pred_random)\n",
    "r2_rf = r2_score(y_test, y_pred_random)\n",
    "print(f\"Best Random Forest Parameters: {rf_random.best_params_}\")\n",
    "print(f\"Mean Squared Error: {mse_rf:.4f}\")\n",
    "print(f\"RÂ² Score: {r2_rf:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b3b80f",
   "metadata": {},
   "source": [
    "## XGBoost Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "18b91a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best XGBoost Parameters: {'subsample': 0.8, 'n_estimators': 300, 'max_depth': 5, 'learning_rate': 0.1, 'colsample_bytree': 1.0}\n",
      "Mean Squared Error: 0.0014\n",
      "RÂ² Score: 0.7749\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist_xgb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "xgb_random = RandomizedSearchCV(\n",
    "    XGBRegressor(random_state=42),\n",
    "    param_distributions=param_dist_xgb,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "xgb_random.fit(X_train, y_train)\n",
    "best_xgb = xgb_random.best_estimator_\n",
    "y_pred_xgb = best_xgb.predict(X_test)\n",
    "# Evaluation\n",
    "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
    "print(f\"Best XGBoost Parameters: {xgb_random.best_params_}\")\n",
    "print(f\"Mean Squared Error: {mse_xgb:.4f}\")\n",
    "print(f\"RÂ² Score: {r2_xgb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4e5dc1",
   "metadata": {},
   "source": [
    "## LightGBM Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d5f8c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000285 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1027\n",
      "[LightGBM] [Info] Number of data points in the train set: 2929, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 0.027435\n",
      "Best LightGBM Parameters: {'subsample': 1.0, 'num_leaves': 50, 'n_estimators': 300, 'max_depth': -1, 'learning_rate': 0.1, 'colsample_bytree': 0.8}\n",
      "Mean Squared Error: 0.0005\n",
      "RÂ² Score: 0.9215\n"
     ]
    }
   ],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "param_dist_lgb = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7, -1],\n",
    "    'num_leaves': [31, 50, 100],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "}\n",
    "\n",
    "lgb_random = RandomizedSearchCV(\n",
    "    LGBMRegressor(random_state=42),\n",
    "    param_distributions=param_dist_lgb,\n",
    "    n_iter=20,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "lgb_random.fit(X_train, y_train)\n",
    "best_lgb = lgb_random.best_estimator_\n",
    "y_pred_lgb = best_lgb.predict(X_test)\n",
    "# Evaluation\n",
    "mse_lgb = mean_squared_error(y_test, y_pred_lgb)\n",
    "r2_lgb = r2_score(y_test, y_pred_lgb)\n",
    "print(f\"Best LightGBM Parameters: {lgb_random.best_params_}\")\n",
    "print(f\"Mean Squared Error: {mse_lgb:.4f}\")\n",
    "print(f\"RÂ² Score: {r2_lgb:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b42018b",
   "metadata": {},
   "source": [
    "## Tommorrows price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee8018f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“… Based on yesterdayâ€™s data (2025-06-27),\n",
      "ðŸ”¹ Linear Regression predicted Dogecoin price: $0.157493\n",
      "ðŸ”¹ Random Forest predicted Dogecoin price: $0.163134\n",
      "ðŸ”¹ XGBoost predicted Dogecoin price: $0.169618\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000087 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 1027\n",
      "[LightGBM] [Info] Number of data points in the train set: 2929, number of used features: 6\n",
      "[LightGBM] [Info] Start training from score 0.027435\n",
      "ðŸ”¹ LightGBM predicted Dogecoin price: $0.161885\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Step 1: Load & Sort Data\n",
    "df = pd.read_csv('Dataset/doge_dataset_day_ohlcvm.csv', parse_dates=['date'])\n",
    "df = df.sort_values('date')\n",
    "\n",
    "# Step 2: Create Lag Feature for Prediction\n",
    "last = df.iloc[-1]\n",
    "X_tomorrow = pd.DataFrame([[\n",
    "    last['market_cap'], \n",
    "    last['volume'], \n",
    "    last['volume_DOGE'], \n",
    "    last['open'], \n",
    "    last['high'], \n",
    "    last['low']\n",
    "]], columns=[\n",
    "    'market_cap_lag1', 'volume_lag1', 'volume_DOGE_lag1', \n",
    "    'open_lag1', 'high_lag1', 'low_lag1'\n",
    "])\n",
    "\n",
    "# Step 3: Build Lagged Dataset\n",
    "df['market_cap_lag1'] = df['market_cap'].shift(1)\n",
    "df['volume_lag1'] = df['volume'].shift(1)\n",
    "df['volume_DOGE_lag1'] = df['volume_DOGE'].shift(1)\n",
    "df['open_lag1'] = df['open'].shift(1)\n",
    "df['high_lag1'] = df['high'].shift(1)\n",
    "df['low_lag1'] = df['low'].shift(1)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Step 4: Split Data\n",
    "features = ['market_cap_lag1', 'volume_lag1', 'volume_DOGE_lag1', \n",
    "            'open_lag1', 'high_lag1', 'low_lag1']\n",
    "X = df[features]\n",
    "y = df['close']\n",
    "train_size = int(len(df) * 0.7)\n",
    "X_train, y_train = X.iloc[:train_size], y.iloc[:train_size]\n",
    "\n",
    "# Step 5: Train All Models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=200, min_samples_split=2, min_samples_leaf=1, max_depth=10),\n",
    "    'XGBoost': XGBRegressor(n_estimators=100, learning_rate=0.1, random_state=42),\n",
    "    'LightGBM': LGBMRegressor(n_estimators=300, learning_rate=0.1, random_state=42)\n",
    "}\n",
    "\n",
    "# Step 6: Fit and Predict for Today\n",
    "print(f\"ðŸ“… Based on yesterdayâ€™s data ({last['date'].date()}),\")\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    predicted_price = model.predict(X_tomorrow)[0]\n",
    "    print(f\"ðŸ”¹ {name} predicted Dogecoin price: ${predicted_price:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
